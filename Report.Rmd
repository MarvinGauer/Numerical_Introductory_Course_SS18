---
title: "Numerical Integration"
subtitle: "Numerical Introductory Course"
author: "Marvin Gauer (580553)"
header-includes:
- \usepackage{titling}
- \usepackage{amsmath}
- \usepackage{xcolor}
- \usepackage{hyperref}
- \usepackage{framed}
- \definecolor{shadecolor}{gray}{0.95}
- \usepackage{animate}
- \usepackage{rotating}
- \newcommand*{\secref}[1]{Section~\ref{#1}}
- \pretitle{\begin{center}\LARGE\includegraphics[width=5cm]{husiegel_bw.png}\\[\bigskipamount]}
- \title{\vspace{3cm} \Huge Numerical Integration \vspace{3cm}}
- \preauthor{\centering \Large Numerical Introductory Course \\}
- \author{\Large Marvin Gauer (580553)}
- \posttitle{\end{center}}
output: pdf_document
fig_caption: true
keep_tex: yes
fontsize: 12pt
citation_package: natbib
bibliography: biblio.bib
nocite: | 
  @Clausthal
  @Kythe
  @Press
---
```{r include = TRUE, echo = FALSE, comment = NA}
knitr::opts_chunk$set(fig.width=6, fig.height=2)
```
```{r include = FALSE}
rm(list = ls(all = TRUE))
graphics.off()

libraries = c("ggplot2", "gridExtra", "nortest")
lapply(libraries, function(x) 
  if (!(x %in% installed.packages())) {
    install.packages(x)
  }
)
lapply(libraries, library, quietly = TRUE, character.only = TRUE)

```

```{r message = FALSE, warning=FALSE, echo=FALSE}
u    = 4     # Upper Boundary
l    = -4    # Lower Boundary

# Function to numerically integrate
pol  = function(x){
  y  = x^2 + 3*x + 4
  return(y)
}

n    = 100 # Max Number of Iterations
m    = 10 # Number of Bins

sigmoid = function(params, x) {
  params[1] / (1 + exp(-params[2] * (x - params[3])))
}

inversefit = function(params,x){
  params[3] - log(x^(-1)-params[1])/(params[2])
}

Crude_MonteCarloIntegration = function(l = NULL, u = NULL, FUN = dnorm, n = 100, m = 1, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations per bin
  # and m is the number of bins
  
  x = seq(l,u,0.01)
  
  # create DataFrame
  df = data.frame(1.5*x, y = FUN(1.5*x))
  
  # create equidistant breaks
  l      = max(x)-min(x)
  step   = l/m
  breaks = seq(min(x),max(x),step)
  
  # Generate Random Points
  mcp_x = runif(n*m, min = min(x), max = max(x))
  
  # Calc corresponding y's
  mcp_y = FUN(mcp_x)
  
  # Calc y's mean within bins
  dfp   = data.frame(mcp_x, mcp_y, "Mean" = vector(length = length(mcp_y)))
  means = vector(length = length(breaks)-1)
  
  for(i in 1:length(breaks)-1){
    means[i] = mean(dfp$mcp_y[dfp$mcp_x >= breaks[i] & dfp$mcp_x <= breaks[i+1]])
    dfp$Mean[dfp$mcp_x >= breaks[i] & dfp$mcp_x <= breaks[i+1]] = means[i]
  }
  
  if (graphic == TRUE){
    
    rect = data.frame(xr = breaks[2:length(breaks)],
                      xl = breaks[1:length(breaks)-1],
                      yu = rep(0,length(breaks)-1),
                      yo = means)
    
    # Visualization of the graph
    p = ggplot(aes(df[,1], df[,2]), data=df) +
      geom_line(size = 1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      ylab("f(y)") + xlab("x") +
      geom_point(data = dfp, aes(x = mcp_x, y = mcp_y, col = 'blue'),size = 3,inherit.aes = F, shape = 20, show.legend = F) + 
      geom_point(data = dfp, aes(x = mcp_x, y = rep(0,length(mcp_x)), col = 'green'),size = 3,inherit.aes = F, shape = 20, show.legend = F) + 
      geom_rect(data=rect, inherit.aes = F,aes(xmin=rect$xl, 
                               xmax=rect$xr, 
                               ymin=rect$yu, 
                               ymax=rect$yo),
                fill = 'red', alpha = 0.2, col = 'red')
    
    print(p)
  }
  
  sol = rep(step,length(means)) %*% means
  
  return(sol)
}

Crude_MonteCarloIteration = function(l = NULL, u = NULL, FUN = dnorm, n = 100, m = 1, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations (min is 10) per bin
  # m is the number of bins and is fixed
  
  x = seq(l,u,0.01)
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(10,n,10)){
    dfg[nrow(dfg) + 1,] = c(i,
                            Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, m = m, graphic = F),
                            integrate(pol,min(x),max(x))$value,
                            integrate(pol,min(x),max(x))$value - Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, m = m, graphic = F))
  }
  
  if (graphic == TRUE){
    
    # Visualization of the Function
    Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = n, m = m, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

Hit_Miss_MonteCarloIntegration = function(l = NULL, u = NULL, FUN = dnorm, n = 100000, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations
  
  x = seq(l,u,0.01)
  
  # create DataFrame
  df = data.frame(1.5*x, y = FUN(1.5*x))
  
  # Generate Random Points
  mcp_x = runif(n, min = min(x), max = max(x))
  mcp_y = runif(n, min = 0, max = max(FUN(x)))
  
  # Calculate the area
  area = (max(x) - min(x))*(max(FUN(x)))
  
  # Approx. Area
  dfp = data.frame(mcp_x, mcp_y)
  dfp["Within"] = ifelse(dfp$mcp_y <= FUN(dfp$mcp_x), TRUE, FALSE)
  
  # Percentage of points <= function
  perc = length(dfp$mcp_x[dfp["Within"] == TRUE])/n
  
  if (graphic == TRUE){
    # Visualization of the graph
    p = ggplot(aes(df[,1], df[,2]), data=df) +
      geom_line(size = 1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_point(aes(x=mcp_x, y=mcp_y, colour = Within), data = dfp, size = 1, show.legend=F) + 
      ylab("f(y)") + xlab("x") 
    
    print(p)
  }
  
  sol = list(as.integer(n),area * perc)
  names(sol) = c("Iterations","Area")
  
  return(sol)
}

Hit_Miss_MonteCarloIteration = function(l = NULL, u = NULL, FUN = dnorm, n = 100000, graphic = TRUE){
  
  # l,u see other functions
  # x contains the x values and FUN is a function
  # n is an integer and represents the number of iterations (min is 10)
  
  x = seq(l,u,0.01)
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(10,n,10)){
    dfg[nrow(dfg) + 1,] = c(i,
                            Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, graphic = F)$Area,
                            integrate(pol,min(x),max(x))$value,
                            integrate(pol,min(x),max(x))$value - Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = pol, n = i, graphic = F)$Area)
  }
  
  if (graphic == TRUE){
    
    # Visualization of the Function
    Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = pol, n = n, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

MidpointIntegration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  x_mid = head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1)
  y_mid = FUN(x_mid)
  
  # create DataFrame
  df   = data.frame(seq(l,u,(u-l)/(n-1)), y = FUN(seq(l,u,(u-l)/(n-1))))
  rect = data.frame(xl = seq(l,u,(u-l)/(n-1))[1:length(seq(l,u,(u-l)/(n-1)))-1], xr = seq(l,u,(u-l)/(n-1))[2:length(seq(l,u,(u-l)/(n-1)))], yu = rep(0,length(seq(l,u,(u-l)/(n-1)))-1), yo = y_mid )
  if (graphic == TRUE){
    # Visualization of the approximation
    g = ggplot() +
      geom_line(aes(x = df[,1], y = df[,2]), data=df) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_rect(data=rect, aes(xmin=rect[,1], 
                               xmax=rect[,2], 
                               ymin=rect[,3], 
                               ymax=rect[,4]),
                fill = 'red', alpha = 0.2, col = 'blue') +
      xlab("x") + ylab("f(x)")
    
    print(g)
    
  }
  
  sol   = sum(y_mid %*% diff(seq(l,u,(u-l)/(n-1))))
  
  return(sol)
}

MidpointIteration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  steps = length(head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1))
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(2,steps,1)){
    dfg[nrow(dfg) + 1,] = c(i,
                            MidpointIntegration(l = l, u = u, n = i, FUN = FUN, graphic = F),
                            integrate(pol,l,u)$value,
                            integrate(pol,l,u)$value - MidpointIntegration(l = l, u = u, n = n, FUN = FUN, graphic = F))
  }
  if (graphic == TRUE){
    # Visualization of the Function
    MidpointIntegration(l = l, u = u, n = n, FUN = FUN, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

SimpsonIntegration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  x_mid = head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1)
  y_mid = FUN(x_mid)
  
  # create DataFrame
  df   = data.frame("interval" = 1:length(x_mid), "x_i" = seq(l,u,(u-l)/(n-1))[1:(n-1)],
                    "x_mid" = x_mid, "x_i+1" = seq(l,u,(u-l)/(n-1))[2:(n)],
                    "y_i" = FUN(seq(l,u,(u-l)/(n-1))[1:(n-1)]), "y_mid" = FUN(x_mid),
                    "y_i+1" = FUN(seq(l,u,(u-l)/(n-1))[2:(n)]))
  
  # Function to fit: y = a*x^2 + b*x + c 
  
  para  = list()
  fct   = list()
  fct_y = list()
  fct_x = list()
  area  = list()
  
  for(i in 1:length(x_mid)){
    A = matrix(c(df$x_i[i]^2, df$x_i[i], 1,
                 df$x_mid[i]^2, df$x_mid[i], 1, 
                 df$x_i.1[i]^2, df$x_i.1[i],1), ncol = 3, byrow = T)
    y = c(df$y_i[i],df$y_mid[i],df$y_i.1[i])
    
    z = solve(A) %*% y
    
    para[[i]] = z
    fct[[i]]  = function(x, c = z){
      v = c[1] * x^2 + c[2] * x + c[3]
      return(v)
    }
    fct_y[[i]] = sapply(seq(df$x_i[i],df$x_i.1[i],0.01), fct[[i]], c = z)
    fct_x[[i]] = seq(df$x_i[i],df$x_i.1[i],0.01)
    
    #fct_y[[i]] = sapply(c(df$x_i[i],df$x_mid[i], df$x_i.1[i]), fct[[i]], c = z)
    #fct_x[[i]] = c(df$x_i[i],df$x_mid[i], df$x_i.1[i])
    area[i]   = integrate(fct[[i]],lower = df$x_i[i],upper = df$x_i.1[i])$value
  }
  
  df1    = data.frame(seq(l,u,0.01), y = FUN(seq(l,u,0.01)))
  df_fct = data.frame(matrix(c(unlist(fct_x),unlist(fct_y)), ncol = 2, byrow=F))
  
  
  if (graphic == TRUE){
    # Visualization of the approximation
    g = ggplot() +
      geom_line(aes(x = df1[,1], y = df1[,2]), data=df1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      xlab("x") + ylab("f(x)") + 
      geom_line(aes(x = df_fct[,1], y = df_fct[,2], col = 'red'), data=df_fct, show.legend = F)
      df2 = data.frame(x = seq(l,u,(u-l)/(n-1))[1:n])
      for(t in 1:length(df2$x)){
        g = g + geom_vline(xintercept = df2$x[t], linetype='dashed', alpha = 0.4)
      }
    print(g)
    
  }
  
  sol = sum(unlist(area))
  return(sol)
}

```

\thispagestyle{empty}
\clearpage
\thispagestyle{empty}

\textbf{Abstract}

Numerical Integration is a widely used numerical method. It is used by practioners all around the globe and in lots of disciplines. 
Due to its relevance, I want to give you a brief overview over some basic numerical integration methods, namely the midpoint rule, simpson rule, crude and hit or miss monte carlo as well as an introduction to the idea of adaptive quadrature and the adaptive simpson routine. 
Furthermore, I want to show how one may use numerical integration methods to approximate the cumulative distribution function of a normal distribution. Here we will see, that even tough just basic methods are applied, one can easily approximate the function so well, that some of the most common normality tests do not find enough evidence to reject the null hypothesis of a normal distribution.

\thispagestyle{empty}
\clearpage
\thispagestyle{empty}
\tableofcontents
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

# 1. Motivation

Integration is an important operation in mathematics. Unfortunately, in real life applications one might find it extremely difficult or even impossible to solve certain integrals in a closed form. Due to the continous improvement in computational power one might address this issue by numercially approximating the integral of interest. In order to do so, several procedures have been developed, each with it's own advantages respectively disadvantages.

# 2. Literature Review {#sec:literature}

Most of the sources used for this document are textbooks. This is due to the fact that the majority of procedures described respectivly used in this document are already well established and recently published papers on numerical integration usually deal with very specific applications and are therefore not very helpful in case of an introduction. 
One of the main sources of this document is [@Davis]. The book contains mathematically very well and precisely defined procedures and algorithms and is most often also cited by other authors from the field of numerical integration. Other well written sources used which explicitly describe Monte Carlo methods are [@Gentle] and 
[@Rizzo]. Even though numerical integration is not the main topic of these books they still describe monte carlo methods and the application of numerical integration very well.
Even tough the papers are quite old [@Lyes] and [@Kuncir] are still interesting to read especially if you are new to the topic. Both deal with the adaptive simpson quadrature elaborated in [Section 3.2.3.](#sec:bib)

# 3. Theory

In the following section I want to elaborate on some of the most well known methods in numerical integration. These can be distinguished into one and multi-dimensional methods. Furthermore one might distinguish these numerical integration techniques into deterministic and probabilistic methods. But before I start introducing the methods of interest I will do a little recap of the Riemann integral, since we are assuming throughout this document, that our functions of interest are Riemann integrable.

Please note, that for the illustrations of the techniques described we will use $f:[-4;4]\rightarrow \mathbb{R}$ with $f(x) = x^2 + 3 \cdot x + 4$ for the midpoint and monte carlo methods, $g:[-4;4]\rightarrow \mathbb{R}$ with $g(x) = x^4 - 3 \cdot x + 4$ for the simpson rule and and $h:[-10;3]\rightarrow \mathbb{R}$ with $g(x) = sin(e^x)$ for the adaptive simpson quadrature.

## 3.1 Review: The Riemann Integral

The Riemann integral is one of the two classic concepts of integrals in analysis. It is named after the German mathematician Bernhard Riemann and it's aim is to calculate the area between the $x$-axis and a certain limited function $f:[a;b] \rightarrow \mathbb{R}$. Loosley speaking, the basic idea behind the concept is to approximate the desired integral by summing up different areas of easier to compute rectangles.

The kind of definition I want to present here is the definition using upper and lower sums introduced by Jean Gaston Darboux:

Let $f:[a;b] \rightarrow \mathbb{R}$ be a limited function and $[a;b]$ be an interval. Furthermore, let $P$ be a partition of $[a;b]$ where $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$. Then we can define the upper and lower sums accordingly:

\begin{equation*}
U(P) = \sum_{k=1}^{n} ((x_k-x_{k-1})\cdot \sup_{x_{k-1} < x < x_{k}} f(x))
\end{equation*}
\begin{equation*}
L(P) = \sum_{k=1}^{n} ((x_k-x_{k-1})\cdot \inf_{x_{k-1} < x < x_{k}} f(x))
\end{equation*}

Now we can compute the infimum and supremum of the upper and lower sum over all partitions $P$. Therefore it follows:

\begin{equation*}
\sup_P L(P) \leq \inf_P U(P)
\end{equation*}

In case of equality, one says that $f$ in Riemann integrable. 

## 3.2 One-Dimensional Procedures

The one dimensional procedurees elaborated on in this chapter are classified as deterministic methods.

### 3.2.1 Midpoint Quadrature

The idea of the midpoint quadrature directly derives from the definition of the Riemann integral. We therefore want to calculate the area between the $x$-axis and a limited function $f:[a;b] \rightarrow \mathbb{R}$.
The algorithm works in the way, that we start by partitioning our interval of interest $[a;b]$ into equidistant subintervals $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$ with stepwidth $h = \frac{b-a}{n}$. Afterwards we calculate the midpoint $x^{(i)}$ within each subinterval $[x_i;x_{i+1}]$ for $i \in \{ 0,1,...,n-1\}$ and evaluate $f$ for each $x^{(i)}$.
For our approximation it then holds that $\int_a^bf(x)dx \approx \sum_{k=0}^{n-1} f(x^{(i)}) \cdot (x_{i+1} - x_{i})$.

An illustration of the procedure can be found in Figure \ref{fig:figs}. \newpage

```{r figs, fig.cap="\\label{fig:figs} Illustration of the Midpoint or Rectangular Quadrature",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
MidpointIntegration(l = l, u = u, n = n/5, FUN = pol, graphic = T)
```

When numerically solving an integral one is naturally interested in the error of the approximation which will in the following be denoted by $E(f)$. According to [@Davis, pp. 54], in case of $f$ having a continous second derivative on $[a;b]$ meaning $f \in C_{[ a;b]}^{(2)}$, an upper bound for the error of the midpoint quadrature can be specified as follows:

\begin{equation*}
\vert E(f)\vert \leq \frac{(b-a)^3}{24\cdot n^2} \cdot \max_{a\leq x \leq b}\vert f^{``}(x)\vert
\end{equation*}

The midpoint rule is exact for linear functions.
Please note, that $E(f)$ might also be used up front to determine the needed number of subintervals $n$ in order to not exceed a certain level of accuracy.

### 3.2.2 Simpson-Rule

The Simposn-Rule is similiar to the Midpoint Quadrature, but instead of rectangles quadratic functions are used in order to calculate the area between the $x$-axis and our limited function $f:[a;b] \rightarrow \mathbb{R}$ more accurately.
We again start by partitioning our interval of interest $[a;b]$ into $n$ subintervals $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$ with equidistant distances which we will in the following denote by $\Delta x = \frac{b-a}{n}$. Afterwards we calculate the midpoint $x^{(i)}$ within each subinterval $[x_i;x_{i+1}]$ for $i \in \{ 0,1,...,n-1\}$.
Now we use the 3 points $\left( x_i; f(x_i)\right)$, $\left( x^{(i)}; f(x^{(i)})\right)$ and $\left( x_{i+1}; f(x_{i+1})\right)$ within each subinterval to interpolate our quadratic functions $g_i(x):[x_i;x_{i+1}] \rightarrow \mathbb{R}$. 
For our approximation it then holds that $\int_a^bf(x)dx \approx \frac{\Delta x}{6} \cdot \left( f(x_0) + 2 \cdot \sum_{k=1}^{n-1} f(x_k) + f(x_n) + 4 \cdot \sum_{k=0}^{n-1} f(x^{(k)})\right)$. \newpage

```{r figs1, fig.cap="\\label{fig:figs1} Illustration of the Simpson Rule",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
pol3 = function(x){
  y = x^4 - 3 * x + 4
  return(y)
}

SimpsonIntegration(l = -4, u = 4, n = 3, FUN = pol3, graphic = T)
```

The red lines in Figure \ref{fig:figs1} are our interpolated quadratic functions, the dashed lines are our borders of the subintervals and the black line is the graph of our function of interest.
As before, we must know how good (or bad) the approximation might be. The error of this approximation will in the following be denoted by $E(f)$. Again, according to [@Davis, pp. 57], in case of $f$ having a continous fourth derivative on $[a;b]$ meaning $f \in C_{[a;b]}^{(4)}$, an upper bound for the error of the Simpson's quadrature can be specified as follows:

\begin{equation*}
\vert E(f) \vert \leq \frac{(b-a)^5}{2880 \cdot n^4} \cdot \max_{a\leq x \leq b}\vert f^{(4)}(x)\vert 
\end{equation*}

The Simpson rule is exact for all polynomials of degree three or less. As in the case of the midpoint quadrature, one might use $E(f)$ to determine the needed number of subintervals $n$ in order to not exceed a certain threshold of error before applying the algorithm.

### 3.2.3 Adaptive Algorithm {#sec:adap}

When calculating integrals numerically one might in some cases not have unlimited computational power. In this case it might not be smart to use an equidstant stepwidth in your numerical integration method. In this case it might be clever to use a wider stepwidth in an area where we can approximate our integral well and a narrower one where our integral cannot be approximated that well.
The adaptive algorithm solves exactly that issue by choosing the subintervals based on the local behavior of the integrand.

One example of an adaptive algorithm would be the adaptive simpson quadrature. There are several versions of this adaptive routine, but the one I want to elaborate on is the one published by @Lyes. Below you can find the pseudocode for a limited function $f:[a;b]\rightarrow \mathbb{R}$. \newpage

\begin{shaded*}
Adaptive($f$, $[a;b]$, $\varepsilon$, $I \approx \int_a^bf(x)dx$)
\begin{itemize}
\item[] Set $m = \frac{a+b}{2}$
\item[] Calculate $I_1 \approx \int_{a}^{m} f(x) dx$ and $I_2 \approx \int_{m}^{b} f(x) dx$ using composite simpson quadrature
\item[] If $ \vert I_1 + I_2 - I  \vert \leq 15 \cdot \varepsilon$ then
\begin{itemize}
\item[] Return $I_1 + I_2 + \frac{I_1 + I_2 - I}{15}$
\end{itemize}
\item[] Else
\begin{itemize}
\item[] Return Adaptive($f$, $[a;m]$, $\frac{\varepsilon}{2}$,$I_1 \approx \int_{a}^{m} f(x) dx$) + Adaptive($f$, $[m;b]$, $\frac{\varepsilon}{2}$, $I_2 \approx \int_{m}^{b} f(x) dx$)
\end{itemize}
\end{itemize}
\end{shaded*}

Please note, that an implementation for all described methods including the adaptive simpson rule can be found in the Quantlet (\href{https://github.com/MarvinGauer/Numerical_Introductory_Course_SS18}{\includegraphics[scale = 0.5]{qletlogo.pdf}}) provided with this document.
Furthermore it may be noted, that the crucial parts of any adaptive algorithm are the quadrature method (e.g. Simpson in the upper case) and the corresponding error estimator applied (e.g. the one suggested by @Lyes). Visualized the procedure looks as follows:  

```{r figs2, fig.cap="\\label{fig:figs2} Example partition of [a;b] in the adaptive simpson quadrature",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
u2    = 3     # Upper Boundary
l2    = -10    # Lower Boundary

# Function to numerically integrate
pol2  = function(x){
  y = sin(exp(x)) 
  return(y)
}

# Graph adaptive

#b = c(l2,0,seq(0.375,5,0.375),u2)
#x_mid = head(filter(b,c(0.5,0.5)),-1)
#y_mid = pol2(x_mid)

# create DataFrame
df     = data.frame(seq(l2,u2,0.01), y = pol2(seq(l2,u2,0.01)))

#rect = data.frame(xl = b[1:length(b)-1], xr = b[2:length(b)], yu = rep(0,length(b)-1), yo = y_mid )
# Visualization of the approximation
g = ggplot() +
  geom_line(aes(x = df[,1], y = df[,2]), data=df) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
  geom_vline(xintercept = c(-10,-5,0.4515827,2.061021,2.648807,3), linetype='dashed', alpha = 0.4) +
  #geom_rect(data=rect, aes(xmin=rect[,1], 
  #                         xmax=rect[,2], 
  #                         ymin=rect[,3], 
  #                         ymax=rect[,4]),
  #          fill = 'red', alpha = 0.2, col = 'blue') +
  xlab("x") + ylab("f(x)")

print(g)
```

In the illustration above we can see the function $f(x) = sin(exp(x))$ in black and the borders of each subinterval denoted by the dashed lines.

According to [@Rice] the adaptive algorithms work just as fine as composite algorithms in case of "well behaved" functions, but outperform when we want to numerically integrate "badly behaved" functions. One example of functions that are not "well behaved" and require for an adaptive quadrature technique are functions which vary rapidly in one part and slowly in another. The interested reader finds many more adaptive routines in [@Davis, chapter 6].

## 3.3 Multi-Dimensional Procedures

In lots of applications multidimensional integrals need to be solved numerically. This means that we now want to calculate the integral $\int_{\Omega}f(\mathbf{x})d\mathbf{x} = \int_{a_1}^{b_1}...\int_{a_m}^{b_m}f(x_1,x_2,...,x_m)dx_1dx_2...dx_m$.
One might have the idea, to use the quadrature rules explained above in a multidimensional setting to calculate the integral at hand, but we can easily see that this will end in the \textbf{curse of dimensionality} (see [appendix](#sec:appendix) for an example illustration). Therefore I want to concentrate on monte carlo integration methods going on.
It may be noted, that both methods presented here result in unbiased estimators of our integral of interest.
Please note that the goodness of your approximation depends on your pseudorandom number generator when applying monte carlo integration methods. Furthermore, for illustration we use one-dimensional integrals for simplicity, but according to 
[@Gentle, page 233] Monte Carlo integration methods should ordinarily only be used for multi-dimensional integrals.

### 3.3.1 Crude

The crude monte carlo integration method is a fairly simple method to calculate integrals numerically.
One needs to generate n m-dimensional uniformally distributed points $\mathbf{x_i} = \left( x_{i,1},...,x_{i,m} \right)$ $\forall i \in \{1,...,n\}$. These points are uniformly distributed on $[a_1;b_1]\times ... \times[a_m;b_m]$. In the next step, we calculate $f(\mathbf{x_i})$ $\forall i \in \{1,...,n\}$ and the corresponding mean $s = \frac{1}{n}\sum_{i=1}^{n}f(\mathbf{x_i})$.
Therefore it holds that $\int_a^bf(\mathbf{x})d\mathbf{x} \approx s \cdot \left( \prod_{l=1}^{m}\left( b_{l} - a_{l}\right)\right)$

```{r figs3, fig.cap="\\label{fig:figs3} Illustration of the Crude Monte Carlo integration method",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
Crude_MonteCarloIntegration(l = l, u = u, n = 100, FUN = pol, graphic = T)
```

As stated before, monte carlo is a probabilistic approach and therefore the questions of bounds for the error of approximation does not arise. Instead of error bounds, we use the variance of the random estimator to indicate the extent of the uncertainty in
our approximation.
Let $\hat{\theta}_{CMC} = s \cdot \left( \prod_{l=1}^{m}\left( b_{l} - a_{l}\right)\right)$ be our estimator for the integral and $V = \prod_{l=1}^{m}\left( b_{l} - a_{l}\right)$ be the volume of our base area. Then the following holds according to [@Gentle, page 231]:

\begin{equation*}
\mathbb{V}[\hat{\theta}_{CMC}] = \frac{V^2}{n}\mathbb{V}[f] \hspace{1cm} with \hspace{1cm} \mathbb{V}[f] = \frac{1}{n-1} \sum_{i=1}^{n}(f(\mathbf{x_i})-s)^2
\end{equation*}

According to [@Davis, pp. 388 - 417] crude monte carlo exists as a last resort, especially for integrals over nonstandard domains or integrands of low order continutity. Another main advantage is, that the rate of convergence is independent of the smoothness of the integrand.

### 3.3.2 Hit or Miss

The hit or miss monte carlo integration method works slidly differently compared to the crude monte carlo integration. Here we create $n$ $m+1$-dimensional uniformly distributed points $\left( x_{i,1},...,x_{i,m},y_i\right)$ for $i \in \{1,..,n\}$. The first $m$ dimensions are for our domain $\left[ a_{1};b_{1}\right] \times \left[ a_{2} \times b_{2}\right] \times ... \times \left[ a_{m};b_{m}\right]$ and the $j^{th}$ coordinate is uniformaly distributed on $\left[ a_{j};b_{j}\right]$ $\forall j \in \{1,...,m\}$. The $y_i$ coordinate corresponds to the functions range and is uniformally distributed on $\left[ 0;max(f(\mathbf{x}))\right]$.
Now the percentage of points $p$ for which holds $y_i \leq f(x_{i,1},...,x_{i,m})$ and the total area/volume $V$ surrounding the graph needs to be calculated.
Therefore it holds that $\int_a^bf(\mathbf{x})dx \approx V \cdot p$ with $V = max(f(\mathbf{x})) \cdot \prod_{l=1}^{m}\left( b_{l} - a_{l}\right)$.

```{r figs4, fig.cap="\\label{fig:figs4} Illustration of the Hit or Miss Monte Carlo integration method ",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
Hit_Miss_MonteCarloIntegration(l = l, u = u, n = 1000, FUN = pol, graphic = T)

library(knitr)
IntervalShifter = function(FUN = NULL, b = as.vector(length = 2)){
  
  if(b[1] == -Inf & b[2] == Inf){
    y = function(t){
      z = FUN(t/(1-t^2)) * (1+t^2)/((1-t^2)^2)
      return(z)
    }
  } else if(b[1] == -Inf & b[2] != Inf){
    y = function(t){
      z = FUN(b[2] - ((1-t)/(t))) * (1)/((t^2))
      return(z)
    }
  } else if(b[1] != -Inf & b[2] == Inf){
    y = function(t){
      z = FUN(as.numeric(b[1]) + (t/(1-t))) * (1)/((1-t)^2)
      return(z)
    }
  }
  
  return(y)
}

options("scipen"=100, "digits"=4)

x         = seq(-6,6,1)
bins      = 15*abs(1:length(x))
y         = vector(length=length(x)-1) 
plot_list = list()

for(i in 1:(length(x)-1)){
  y[i]  = MidpointIntegration(l = 0, u = 1, n = bins[i+1], FUN = IntervalShifter(dnorm,b = c(-Inf,x[i+1])), graphic = F)[1]
}

df   = data.frame("x"              = x[2:length(x)], 
                  "Number of Bins" = bins[2:length(bins)], 
                  "y_hat"          = y,
                  "y"              = pnorm(x[2:length(x)]),
                  "error"          = y-pnorm(x[2:length(x)]))

#colnames(df) <- c("x", "Number of Bins","Approx. y", "y","Error")
```

As for the crude monte carlo integration we are also interested in the variance of our estimator $\hat{\theta}_{HM} = V \cdot p$ for which holds:

\begin{equation*}
\hat{\mathbb{V}}[\hat{\theta}_{HM}] = \frac{V^2 \cdot p(1-p)}{n}
\end{equation*}

It may be noted, that according to [@Hammersly, page 54] and [@Gentle, page 232] the crude monte carlo method is the superior method, since both estimators are unbiased, but the variance of $\hat{\theta}_{CMC}$ is always smaller for fixed $n$. \newpage

## 3.4 Integrals over infinite Intervals

Working with the procedures elaborated on above one needs to have certain limits for the integral to approximate, but in many applications one might also be interested in numerically solving improper integrals.
I therefore want to present you some formulas applying a change of variables in order to account for the issue described above. The basic idea of the procedure is to adjust the integrand in such a way, that we have an equivalent definite integral at the end. Therefore we can use the following three functions to trasform our integral at hand:

\begin{equation*}
\int_{-\infty}^{\infty}f(x)dx = \int_{-1}^{1}f(\frac{t}{1-t})\cdot \frac{1+t^2}{(1-t^2)^2}dt
\end{equation*}
\begin{equation*}
\int_{a}^{\infty}f(x)dx = \int_{0}^{1}f(a + \frac{t}{1-t})\cdot \frac{1}{(1-t)^2}dt
\end{equation*}
\begin{equation*}
\int_{-\infty}^{a}f(x)dx = \int_{0}^{1}f(a - \frac{1-t}{t})\cdot \frac{1}{t^2}dt
\end{equation*}

# 4. Application: Approximation of the Normal Distribution

Numerical integration is widely used by practitioners. Some of the most well known respectively used applications are among the following:

\begin{itemize}
\item Approximation of probabilities of the normal distribution
\item Approximation of antiderivatives
\item Calculation of moments
\item Calculation of the Value at Risk and Expected Shortfall
\end{itemize}

The application I want to elaborate on is the approximation of the antiderivative of the normal distribution's density function since it cannot be expressed using only elementary functions. In order to do so I will use the scientific programming language $R$.

For our application we first need to decide on the sampling points $x_i$ for $i=1,...,N$ of our distribution. I choose all integers between -5 and 6. 
Due to the fact, that the function is quite well behaved there is no need to apply an adaptive algorithm. Therefore I will use the midpoint quadrature as a method of choice. Since I need to solve an improper integral, a change of variables is used and furthermore I will apply $15 \cdot i$ bins for each sampling point $x_i$ with $i = {1,..,12}$.\newpage

\begin{figure}
\centering
\animategraphics[loop,autoplay, scale = 0.5]{2}{Test}{001}{012}
\caption{Visualization of the steps of integration}
\label{gif}
\end{figure}
\hfill \href{https://github.com/MarvinGauer/Numerical_Introductory_Course_SS18}{\includegraphics[scale = 0.5]{qletlogo.pdf}}

To approximate $F(x)$ we now need to numerically evaluate the following integral for our $N=12$ sampling points:

\begin{equation*}
\int_{-\infty}^{x_i} f(x) dx \hspace{0.2cm} \forall i \in \{1,..,12\}
\end{equation*}

The obtained points will in the following be denoted by $\hat{y}$. The results of our approximation can be found visualized in Figure \ref{gif} and in written form below:

```{r tab1, echo=FALSE}
kable(df, align = 'r', caption = "\\label{tab:tab1} Results for the 12 sampling points",col.names = c("x", "Number of Bins","$\\hat{y}$", "y","$\\varepsilon$"), digits = 8, escape=F)
```

\newpage
It may be noted, that the $y$ is based on R's pnorm()-function.
Now that we have our sampling points evaluated we need to decide on a suited function to fit the points. In our case I decided to use a sigmoid function that is fitted using a non-linear least squares approach. This then yields: 

\begin{equation*}
\hat{F}(x) = \frac{1}{1+\exp^{-\hat{b} \cdot (x-\hat{c})}} = \frac{1}{1+\exp^{-1.706 \cdot x}}
\end{equation*}

After approximating our function we now need to test the goodness of fit. We will do this by applying a collection of normality tests. I will use the Shapiro-Wilks, Anderson-Darling, Lilliefors and Kolmogorov-Smirnov as tests of choice, even though [@Shapiro] showed, that Shapiro-Wilks is the most powerful of the tests used.
In order to test whether there is evidence that our approximated distribution is not normal, we need to sample datapoints. For this we create 500 uniformly on $\left[0;1\right]$ distributed random samples. Then we use the inverse of our fitted function to have sample datapoints corresponding to the approximated CDF $\hat{F}$. The corresponding results/p-values can be found below.

```{r tab2,echo=FALSE, results = 'asis',comment = NA}
fitmodel <- nls(y~a/(1 + exp(-b * (x[2:length(x)]-c))), start=list(a=1,b=1,c=0),algorithm="port", 
                lower=c(1,0,0), upper=c(1,20,20))
params=coef(fitmodel)

set.seed(7)
y_new  = runif(500,0,1)
x_new  = inversefit(params,y_new)


cdf_plot      = data.frame(x = seq(min(x),max(x),0.01), r_F = pnorm(seq(min(x),max(x),0.01)), e_F = sigmoid(params,seq(min(x),max(x),0.01)), Diff = pnorm(seq(min(x),max(x),0.01))-sigmoid(params,seq(min(x),max(x),0.01)))
sample_points = data.frame(x = inversefit(params,y_new), y = sigmoid(params,x_new))

frame = data.frame("Shapiro-Wilks:" = shapiro.test(x_new)$p.value,
                   "Anderson-Darling:" = ad.test(x_new)$p.value,
                   "Lilliefors:" = lillie.test(x_new)$p.value,
                   "Kolmogorov-Smirnov" = ks.test(x_new,"pnorm")$p.value)

knitr::kable(frame, caption = "\\label{tab:tab2} Results/p-values of the normality tests", 
             col.names = c("Shapiro-Wilks:", "Anderson-Darling:","Lilliefors:", "Kolmogorov-Smirnov:"),floating.environment="sidewaystable")
```


According to the test's p-values we cannot reject the null hypothesis even ones, meaning we could not find enough evidence to conclude, that the datapoints are not normally distributed (corresponding QQ-Plot can also be found in [appendix](#sec:appendix)).
Furthermore we want to plot the approximated and R's pnorm-function. The plot can be found below and we can easily see, that only slight differences can be spotted indicating, that our approximation already fits the normal's CDF quite well.

```{r figs5, fig.cap="\\label{fig:figs5} Visualization of the fitted (red) and R's Normal Distributions CDF (blue)",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center",fig.width=6, fig.height=2.5}
g = ggplot() +
  geom_line(aes(x = x, y = r_F, col = 'black'), data=cdf_plot, show.legend = F) + 
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
  geom_line(aes(x = x, y = e_F, colour = 'red'), data = cdf_plot, show.legend = F) +
  xlab("x") + ylab("F(x)") +
  geom_point(aes(x = x, y = y, colour = 'black'), data = sample_points, shape = 8,show.legend = F)

print(g)
```

The points displayed on the approximated curve $\hat{F}$ are the 500 randomly sampled datapoints.

# 5. Conclusion

The 5 procedures elaborated on in this document are a good basis in numerical integration. In our application one could for example easily see, that even though just basic methods were applied we can already easily retrieve "good" results. It may be noted, that "good" in this sense means, that we are not able to reject the null hypothesis for some of the most well known normaility tests. 
Depending on the application at hand this measure of goodness might not be appropriate and the user therfore might have to use more advanced methods in order to achieve the desired level of goodness respectively accuracy.
Due to the fact that the topic is very important in applications, there were many more procedures developed by the scientific community and each of these comes with its own advantages and disadvantages. For further readings on the topic one might for example proceed with the textbooks elaborated on in the [literature review](#sec:literature) or [bibliography](#sec:bib).

# I Appendix{#sec:appendix}

```{r figs6, fig.cap="\\label{fig:figs6} QQPlot of R's Normal Distributions CDF and the simulated points distributed according to the approximated CDF",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center",fig.width=3, fig.height=3}

g = ggplot(sample_points, aes(sample=x)) + 
  stat_qq() +
  xlab("Theoretical normal quantiles") + ylab("Simulated and approximated data points") +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
  geom_line(aes(x = x, y = y, col = 'black'), data = data.frame(x = seq(-3.5,3.5,0.1), y = seq(-3.5,3.5,0.1)), show.legend = F)

print(g)
```

```{r figs7, fig.cap="\\label{fig:figs7} Example illustration of the curse of dimensionality",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center",fig.width=3, fig.height=2}

# create DataFrame
df   = data.frame(seq(1,7,1), y = seq(1,7,1))
rect = data.frame(xl = c(1,1,1,3,3,3,5,5,5), 
                  xr = c(3,3,3,5,5,5,7,7,7), 
                  yu = c(1,3,5,1,3,5,1,3,5), 
                  yo = c(3,5,7,3,5,7,3,5,7) )

# Visualization of the approximation
g1 = ggplot() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
  geom_rect(data=rect, aes(xmin=rect[,1], 
                           xmax=rect[,2], 
                           ymin=rect[,3], 
                           ymax=rect[,4]),
            fill = 'red', alpha = 0.2, col = 'blue') +
  xlab(expression(paste(x[1]))) + ylab(expression(paste(x[2])))
rect2= data.frame(xl = c(1,1,4,4), 
                  xr = c(4,4,7,7), 
                  yu = c(1,4,1,4), 
                  yo = c(4,7,4,7) )
g2 = ggplot() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), 
        axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
  geom_rect(data=rect2, aes(xmin=rect2[,1], 
                            xmax=rect2[,2], 
                            ymin=rect2[,3], 
                            ymax=rect2[,4]),
            fill = 'red', alpha = 0.2, col = 'blue') +
  xlab(expression(paste(x[1]))) + ylab(expression(paste(x[2]))) +
  xlim(1,7) + ylim(1,7)

grid.arrange(g1,g2,ncol=2)
```

# Bibliography {#sec:bib}

