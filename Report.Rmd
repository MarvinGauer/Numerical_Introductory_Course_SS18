---
title: "Numerical Integration"
subtitle: "Numerical Introductory Course"
author: "Marvin Gauer (580553)"
header-includes:
- \usepackage{titling}
- \usepackage{amsmath}
- \pretitle{\begin{center}\LARGE\includegraphics[width=5cm]{husiegel_bw.png}\\[\bigskipamount]}
- \title{\vspace{3cm} \Huge Numerical Integration \vspace{3cm}}
- \preauthor{\centering \Large Numerical Introductory Course \\}
- \author{\Large Marvin Gauer (580553)}
- \posttitle{\end{center}}

output: pdf_document
fig_caption: true
keep_tex: yes
fontsize: 12pt
citation_package: natbib
bibliography: biblio.bib
---
```{r include = TRUE, echo = FALSE, comment = NA}
knitr::opts_chunk$set(fig.width=2, fig.height=2)
```
```{r include = FALSE}
rm(list=ls())

library(ggplot2)

```
\thispagestyle{empty}
\clearpage
\thispagestyle{empty}
\tableofcontents
\newpage
\pagenumbering{arabic}
\setcounter{page}{1}

```{r message = FALSE, warning=FALSE, echo=FALSE}
u    = 4     # Upper Boundary
l    = -4    # Lower Boundary

# Function to numerically integrate
pol  = function(x){
  y  = x^2 + 3*x + 4
  return(y)
}

n    = 100 # Max Number of Iterations
m    = 10 # Number of Bins

Crude_MonteCarloIntegration = function(l = NULL, u = NULL, FUN = dnorm, n = 100, m = 10, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations per bin
  # and m is the number of bins
  
  x = seq(l,u,0.01)
  
  # create DataFrame
  df = data.frame(1.5*x, y = FUN(1.5*x))
  
  # create equidistant breaks
  l      = max(x)-min(x)
  step   = l/m
  breaks = seq(min(x),max(x),step)
  
  # Generate Random Points
  mcp_x = runif(n*m, min = min(x), max = max(x))
  
  # Calc corresponding y's
  mcp_y = FUN(mcp_x)
  
  # Calc y's mean within bins
  dfp   = data.frame(mcp_x, mcp_y, "Mean" = vector(length = length(mcp_y)))
  means = vector(length = length(breaks)-1)
  
  for(i in 1:length(breaks)-1){
    means[i] = mean(dfp$mcp_y[dfp$mcp_x >= breaks[i] & dfp$mcp_x <= breaks[i+1]])
    dfp$Mean[dfp$mcp_x >= breaks[i] & dfp$mcp_x <= breaks[i+1]] = means[i]
  }
  
  if (graphic == TRUE){
    
    rect = data.frame(xr = breaks[2:length(breaks)],
                      xl = breaks[1:length(breaks)-1],
                      yu = rep(0,length(breaks)-1),
                      yo = means)
    
    # Visualization of the graph
    p = ggplot(aes(df[,1], df[,2]), data=df) +
      geom_line(size = 1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      ylab("f(y)") + xlab("x") +
      geom_point(data = dfp, aes(x = mcp_x, y = mcp_y, col = 'blue'),size = 3,inherit.aes = F, shape = 20, show.legend = F) + 
      geom_point(data = dfp, aes(x = mcp_x, y = rep(0,length(mcp_x)), col = 'green'),size = 3,inherit.aes = F, shape = 20, show.legend = F) + 
      geom_rect(data=rect, inherit.aes = F,aes(xmin=rect$xl, 
                               xmax=rect$xr, 
                               ymin=rect$yu, 
                               ymax=rect$yo),
                fill = 'red', alpha = 0.2, col = 'red')
    
    print(p)
  }
  
  sol = rep(step,length(means)) %*% means
  
  return(sol)
}

Crude_MonteCarloIteration = function(l = NULL, u = NULL, FUN = dnorm, n = 100, m = 10, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations (min is 10) per bin
  # m is the number of bins and is fixed
  
  x = seq(l,u,0.01)
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(10,n,10)){
    dfg[nrow(dfg) + 1,] = c(i,
                            Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, m = m, graphic = F),
                            integrate(pol,min(x),max(x))$value,
                            integrate(pol,min(x),max(x))$value - Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, m = m, graphic = F))
  }
  
  if (graphic == TRUE){
    
    # Visualization of the Function
    Crude_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = n, m = m, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

Hit_Miss_MonteCarloIntegration = function(l = NULL, u = NULL, FUN = dnorm, n = 100000, graphic = TRUE){
  
  # l,u see other functions
  # n is an integer and represents the number of iterations
  
  x = seq(l,u,0.01)
  
  # create DataFrame
  df = data.frame(1.5*x, y = FUN(1.5*x))
  
  # Generate Random Points
  mcp_x = runif(n, min = min(x), max = max(x))
  mcp_y = runif(n, min = 0, max = max(FUN(x)))
  
  # Calculate the area
  area = (max(x) - min(x))*(max(FUN(x)))
  
  # Approx. Area
  dfp = data.frame(mcp_x, mcp_y)
  dfp["Within"] = ifelse(dfp$mcp_y <= FUN(dfp$mcp_x), TRUE, FALSE)
  
  # Percentage of points <= function
  perc = length(dfp$mcp_x[dfp["Within"] == TRUE])/n
  
  if (graphic == TRUE){
    # Visualization of the graph
    p = ggplot(aes(df[,1], df[,2]), data=df) +
      geom_line(size = 1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_point(aes(x=mcp_x, y=mcp_y, colour = Within), data = dfp, size = 1, show.legend=F) + 
      ylab("f(y)") + xlab("x") 
    
    print(p)
  }
  
  sol = list(as.integer(n),area * perc)
  names(sol) = c("Iterations","Area")
  
  return(sol)
}

Hit_Miss_MonteCarloIteration = function(l = NULL, u = NULL, FUN = dnorm, n = 100000, graphic = TRUE){
  
  # l,u see other functions
  # x contains the x values and FUN is a function
  # n is an integer and represents the number of iterations (min is 10)
  
  x = seq(l,u,0.01)
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(10,n,10)){
    dfg[nrow(dfg) + 1,] = c(i,
                            Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = FUN, n = i, graphic = F)$Area,
                            integrate(pol,min(x),max(x))$value,
                            integrate(pol,min(x),max(x))$value - Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = pol, n = i, graphic = F)$Area)
  }
  
  if (graphic == TRUE){
    
    # Visualization of the Function
    Hit_Miss_MonteCarloIntegration(l = l, u = u, FUN = pol, n = n, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

MidpointIntegration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  x_mid = head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1)
  y_mid = FUN(x_mid)
  
  # create DataFrame
  df   = data.frame(seq(l,u,(u-l)/(n-1)), y = FUN(seq(l,u,(u-l)/(n-1))))
  rect = data.frame(xl = seq(l,u,(u-l)/(n-1))[1:length(seq(l,u,(u-l)/(n-1)))-1], xr = seq(l,u,(u-l)/(n-1))[2:length(seq(l,u,(u-l)/(n-1)))], yu = rep(0,length(seq(l,u,(u-l)/(n-1)))-1), yo = y_mid )
  if (graphic == TRUE){
    # Visualization of the approximation
    g = ggplot() +
      geom_line(aes(x = df[,1], y = df[,2]), data=df) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_rect(data=rect, aes(xmin=rect[,1], 
                               xmax=rect[,2], 
                               ymin=rect[,3], 
                               ymax=rect[,4]),
                fill = 'red', alpha = 0.2, col = 'blue') +
      xlab("x") + ylab("f(x)")
    
    print(g)
    
  }
  
  sol   = sum(y_mid %*% diff(seq(l,u,(u-l)/(n-1))))
  
  return(sol)
}

MidpointIteration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  steps = length(head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1))
  
  # Convergence Data.Frame
  dfg = data.frame("Iteration"     = as.integer(), 
                   "Approx. Value" = as.numeric(), 
                   "Real Value"    = as.numeric(),
                   "Difference"    = as.numeric())
  
  for(i in seq(2,steps,1)){
    dfg[nrow(dfg) + 1,] = c(i,
                            MidpointIntegration(l = l, u = u, n = i, FUN = FUN, graphic = F),
                            integrate(pol,l,u)$value,
                            integrate(pol,l,u)$value - MidpointIntegration(l = l, u = u, n = n, FUN = FUN, graphic = F))
  }
  if (graphic == TRUE){
    # Visualization of the Function
    MidpointIntegration(l = l, u = u, n = n, FUN = FUN, graphic = T)
    
    # Visualization of the convergence
    g = ggplot(aes(x = dfg[,1], y = dfg[,2]), data=dfg) +
      geom_line() + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      geom_line(aes(x = dfg[,1], y = dfg[,3], colour = 'red'), data = dfg, show.legend=F) +
      xlab("Iterations") + ylab("Area")
    
    print(g)
  }
  
  return(dfg)
}

SimpsonIntegration = function(l = NULL, u = NULL, n = 10, FUN = dnorm, graphic = T){
  
  # FUN is the function of interest
  
  x_mid = head(filter(seq(l,u,(u-l)/(n-1)),c(0.5,0.5)),-1)
  y_mid = FUN(x_mid)
  
  # create DataFrame
  df   = data.frame("interval" = 1:length(x_mid), "x_i" = seq(l,u,(u-l)/(n-1))[1:(n-1)],
                    "x_mid" = x_mid, "x_i+1" = seq(l,u,(u-l)/(n-1))[2:(n)],
                    "y_i" = FUN(seq(l,u,(u-l)/(n-1))[1:(n-1)]), "y_mid" = FUN(x_mid),
                    "y_i+1" = FUN(seq(l,u,(u-l)/(n-1))[2:(n)]))
  
  # Function to fit: y = a*x^2 + b*x + c 
  
  para  = list()
  fct   = list()
  fct_y = list()
  fct_x = list()
  area  = list()
  
  for(i in 1:length(x_mid)){
    A = matrix(c(df$x_i[i]^2, df$x_i[i], 1,
                 df$x_mid[i]^2, df$x_mid[i], 1, 
                 df$x_i.1[i]^2, df$x_i.1[i],1), ncol = 3, byrow = T)
    y = c(df$y_i[i],df$y_mid[i],df$y_i.1[i])
    
    z = solve(A) %*% y
    
    para[[i]] = z
    fct[[i]]  = function(x, c = z){
      v = c[1] * x^2 + c[2] * x + c[3]
      return(v)
    }
    fct_y[[i]] = sapply(seq(df$x_i[i],df$x_i.1[i],0.01), fct[[i]], c = z)
    fct_x[[i]] = seq(df$x_i[i],df$x_i.1[i],0.01)
    
    #fct_y[[i]] = sapply(c(df$x_i[i],df$x_mid[i], df$x_i.1[i]), fct[[i]], c = z)
    #fct_x[[i]] = c(df$x_i[i],df$x_mid[i], df$x_i.1[i])
    area[i]   = integrate(fct[[i]],lower = df$x_i[i],upper = df$x_i.1[i])$value
  }
  
  df1    = data.frame(seq(l,u,0.01), y = FUN(seq(l,u,0.01)))
  df_fct = data.frame(matrix(c(unlist(fct_x),unlist(fct_y)), ncol = 2, byrow=F))
  
  
  if (graphic == TRUE){
    # Visualization of the approximation
    g = ggplot() +
      geom_line(aes(x = df1[,1], y = df1[,2]), data=df1) + 
      theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
            panel.background = element_blank(), 
            axis.line = element_line(colour = "black", arrow = arrow(length = unit(0.25, "cm")))) +
      xlab("x") + ylab("f(x)") + 
      geom_line(aes(x = df_fct[,1], y = df_fct[,2], col = 'red'), data=df_fct, show.legend = F)
      df2 = data.frame(x = seq(l,u,(u-l)/(n-1))[1:n])
      for(t in 1:length(df2$x)){
        g = g + geom_vline(xintercept = df2$x[t], linetype='dashed', alpha = 0.4)
      }
    print(g)
    
  }
  
  sol = sum(unlist(area))
  return(sol)
}

```

# 1. Motivation

Integration is an important operation in mathematics. Unfortunately, in real life applications one might find it extremely difficult or even impossible to solve certain integrals in a closed form. Due to the continous improvement in computational power one might address this issue by numercially approximating the integral of interest. In order to do so, several procedures have been developed, each with it's own advantages respectively disadvantages.
In this document we want to present 5 methods for numerical integration and furthermore apply one of those methods to numerically integrate the normal distributions density in order to approximate the normals cumulative distribution function.

# 2. Literature Review

lalalala

# 3. Theory

In the following section I want to explain some of the most popular methods in numerical integration. These can be distingushed into one and multi-dimensional methods. Furthermore one might distinguesh numerical integration methods further into deterministic and probabilistic methods. But before I start introducing the methods of interest I will do a little recap of the bascis:

# 3.1 Review: The Riemann Integral

The Riemann Integral is one of the two classic concepts of integrals in analysis. It is named after the German mathematician Bernhard Riemann and it's aim is to calculate the area between the $x$-axis and a certain limited function $f:[a;b] \rightarrow \mathbb{R}$. Loosley speaking, the basic idea behind the concept is to approximate the desired integral by summing up different areas of easier to compute rectangles.

The kind of definition I want to present here is the definition using upper and lower sums introduced by Jean Gaston Darboux:

Let $f:[a;b] \rightarrow \mathbb{R}$ be a limited function and $[a;b]$ be an interval. Furthermore, let $P$ be a partition of $[a;b]$ where $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$. Then we can define the upper and lower sums accordingly:

\begin{equation*}
U(P) = \sum_{k=1}^{n} ((x_k-x_{k-1})\cdot \sup_{x_{k-1} < x < x_{k}} f(x))
\end{equation*}
\begin{equation*}
L(P) = \sum_{k=1}^{n} ((x_k-x_{k-1})\cdot \inf_{x_{k-1} < x < x_{k}} f(x))
\end{equation*}

Now we can compute the infimum and supremum of the upper and lower sum over all partitions $P$. Therefore it follows:

\begin{equation*}
\sup_P L(P) \leq \inf_P U(P)
\end{equation*}

In case of equality, on says that $f$ in Riemann integrable. 

# 3.2 One-Dimensional Procedures

The one dimensional procedurees elaborated on in this chapter are classified as deterministic methods.
Throughout this document the function $f:[-4;4]\rightarrow \mathbb{R}$ with $f(x) = x^2 + 3 \cdot x + 4$ is used for visualizing the procedures introduced.

## 3.2.1 Midpoint Quadrature

The idea of the Midpoint Quadrature directly derives from the definition of the Riemann Integral. We therefore want to calculate the area between the $x$-axis and a limited function $f:[a;b] \rightarrow \mathbb{R}$.
The algorithm works in the way, that we start by partitioning our interval of interest $[a;b]$ into aquidistant subintervals $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$ with stepwidth $h = \frac{b-a}{n}$. Afterwards we calculate the midpoint $x^{(i)}$ within each subinterval $[x_i;x_{i+1}]$ for $i \in \{ 0,1,...,n-1\}$ and evaluate $f$ for each $x^{(i)}$.
For our approximation it then holds that $\int_a^bf(x)dx \approx \sum_{k=0}^{n-1} f(x^{(i)} \cdot (x_{i+1} - x_{i})$.

An illustration of the procedure can be found in Figure \ref{fig:figs}.

```{r figs, fig.cap="\\label{fig:figs} Illustration of the Midpoint or Rectangular Quadrature",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
MidpointIntegration(l = l, u = u, n = n/5, FUN = pol, graphic = T)
```

When numerically solving an integral one is naturally interested in the error of the approximation which will in the following be denoted by $E(f)$. In case of $f$ having a continous second derivative on $[a;b]$ meaning $f \in C_{[ a;b]}^{(2)}$, an upper bound for the error of the Midpoint quadrature can be specified as follows:

\begin{equation*}
E(f) = \frac{(b-a)^3}{24\cdot n^2} \cdot \max_{a\leq x \leq b}\vert f^{``}(x)\vert
\end{equation*}

## 3.2.2 Simpson-Rule

The Simposn-Rule is similiar to the Rectangular Quadrature, but instead of rectangles quadratic functions are used in order to calculate the area between the $x$-axis and our limited function $f:[a;b] \rightarrow \mathbb{R}$ more accurately.
We again start by partitioning our interval of interest $[a;b]$ into $n$ subintervals $a = x_0 < x_1 < ... < x_{n-1} < x_n = b$ with equidistant distances which we will in the following denote by $\Delta x = \frac{b-a}{n}$. Afterwards we calculate the midpoint $x^{(i)}$ within each subinterval $[x_i;x_{i+1}]$ for $i \in \{ 0,1,...,n-1\}$.
Now we use the 3 points $\left( x_i; f(x_i)\right)$, $\left( x^{(i)}; f(x^{(i)})\right)$ and $\left( x_{i+1}; f(x_{i+1})\right)$ within each subinterval to interpolate our quadratic functions $g_i(x):[x_i;x_{i+1}] \rightarrow \mathbb{R}$. 
For our approximation it then holds that $\int_a^bf(x)dx \approx \frac{\Delta x}{6} \cdot \left( f(x_0) + 2 \cdot \sum_{k=1}^{n-1} f(x_k) + f(x_n) + 4 \cdot \sum_{k=0}^{n-1} f(x^{(k)})\right)$. 

```{r figs1, fig.cap="\\label{fig:figs1} Illustration of the Simpson Rule",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
SimpsonIntegration(l = l, u = u, n = n/10, FUN = pol, graphic = T)
```

The Simpson's rule is an approximation. As with any approximation, before you
can safely use it, you must know how good (or bad) the approximation might be.The error of this approximation will in the following be denoted by $E(f)$. In case of $f$ having a continous fourth derivative on $[a;b]$ meaning $f \in C_{[a;b]}^{(4)}$, an upper bound for the error of the Simpson's quadrature can be specified as follows:

\begin{equation*}
E(f) \leq = \frac{(b-a)^5}{2880 \cdot n^4} \cdot \max_{a\leq x \leq b}\vert f^{(4)}(x)\vert 
\end{equation*}

## 3.2.3 Adaptive Algorithm

When calculating integrals numerically one might in some cases not have unlimited computational power. In this case it might not be smart to use aquidstant stepwidth for in your numerical integration method. In this case it mit be clever to use a wider stepwidth in an area where we can approximate our function well and a narrower one where our function cannot be approximated that well.
The adaptive algorithm solves exactly that issue by minimizing the local error (the error within each subinterval) until it reaches a certain error threshold. 
The algorithm therefore starts with an initial number of subintervals $m$. It then approxima

# 3.3 Multi-Dimensional Procedures

In lots of applications multidomensional Inetgrals need to be solved numerically. This means that we now want to calculate the integral $\int_{\Omega}f(\mathbf{x})d\mathbf{x} = \int_{a_1}^{b_1}...\int_{a_m}^{b_m}f(x_1,x_2,...,x_m)dx_1dx_2...dx_m$.
One might have the idea, to use the quadrature rules explained above in a multidimensional setting to calculate the integral at hand, but we can easily see that this will end in the \textbf{Curse of dimensionality}. Therefore I want to concentrate on Monte Carlo integration methods going on.
Please note that the goodness of your approximation depends on your random number generator when applying Monte Carlo integration methods. 

## 3.3.1 Crude

The Crude Monte Carlo Integration method is a fairly simple method to calculate integrals numerically. It works in the way, that we partition $k$ bins $\left[ a_{1,j};b_{1,j}\right] \times \left[ a_{2,j} \times b_{2,j}\right] \times ... \times \left[ a_{m,j};b_{m,j}\right]$ $\forall j \in \{1,...,k \}$.
Furthermore, one needs to generate n m-dimensional uniformally distributed points $\mathbf{x_i} = \left( x_1,...,x_m \right)$ $\forall i \in \{1,...,n\}$. In the next step, we calculate $f(\mathbf{x_i})$ $\forall i \in \{1,...,n\}$ and the means $m_i$ for each bin.
Therefore it holds that $\int_a^bf(x)dx \approx \sum_{t = 1}^{k}\left( \prod_{l=1}^{m}\left( b_{l,t} - a_{l,t}\right)\cdot m_t\right)$

```{r figs2, fig.cap="\\label{fig:figs2} Illustration of the Crude Monte Carlo integration method",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
Crude_MonteCarloIntegration(l = l, u = u, n = 100, m = 10, FUN = pol, graphic = T)
```

## 3.3.2 Hit or Miss

On the other hand, the Hit or Miss Monte Carlo Integration method works slidly differently. Here we create $n$ $m+1$-dimensional uniformly distributed points $\left( x_{i,1},...,x_{i,m},y_i\right)$ for $i \in \{1,..,n\}$. The first $m$ dimensions are for our domain $\left[ a_{1};b_{1}\right] \times \left[ a_{2} \times b_{2}\right] \times ... \times \left[ a_{m};b_{m}\right]$ and are always uniformaly distributed on $\left[ a_{i};b_{i}\right]$ $\forall i \in \{1,...,m\}$. The $y_i$ coordinate is for our $f(\mathbf{x})$-values and these are uniformally distributed on $\left[ 0;max(f(\mathbf{x}))\right]$.
Now the percentage of points $p$ for which holds $y_i \leq f(x_1,...,x_m)$ and the total area/vloume $A$ surrounding the graph needs to be calculated. $A$ is basically the area/volume of the domain of the generated random points.
Therefore it holds that $\int_a^bf(x)dx \approx A \cdot p$

```{r figs3, fig.cap="\\label{fig:figs3} Illustration of the Hit or Miss Monte Carlo integration method ",echo=FALSE,results='hide',fig.pos="h",out.extra='',fig.align="center"}
Hit_Miss_MonteCarloIntegration(l = l, u = u, n = 1000, FUN = pol, graphic = T)
```

Obviously, the quality of the answer depends on the quality of the random number generator sequence which is used.

# 3.4 Integrals over infinite Intervals

Working with the procedures elaborated on above one needs to have certain limits for the integral to approximate, but in many applications one might also be interested in numerically solving improper integrals.
I therefore want to elaborate on a method known as change of variables in order to account for the issue described above. The basic idea of the procedure is to adjust the integrand in such a way, that we have an equivalent definite integral at the end. Therefore we can use the following three functions to trasform our integral at hand:

\begin{equation*}
\int_{-\infty}^{\infty}f(x)dx = \int_{-1}^{1}f(\frac{t}{1-t})\cdot \frac{1+t^2}{(1-t^2)^2}dt
\end{equation*}
\begin{equation*}
\int_{a}^{\infty}f(x)dx = \int_{0}^{1}f(a + \frac{t}{1-t})\cdot \frac{1}{(1-t)^2}dt
\end{equation*}
\begin{equation*}
\int_{-\infty}^{a}f(x)dx = \int_{0}^{1}f(a - \frac{1-t}{t})\cdot \frac{1}{t^2}dt
\end{equation*}
# 4. Application: Approximation of the Normal Distribution

lalalala

# 5. Conclusion

lalalala

# 6. Bibliography

@Clausthal





